{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_i1q1TWG9zH"
   },
   "source": [
    "# Deep Crossentropy method\n",
    "\n",
    "In this section we'll extend your CEM implementation with neural networks! You will train a multi-layer neural network to solve simple continuous state space games. __Please make sure you're done with tabular crossentropy method from the previous notebook.__\n",
    "\n",
    "![img](https://watanimg.elwatannews.com/old_news_images/large/249765_Large_20140709045740_11.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t4CJ1sRyG9zJ",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:41:09.550544Z",
     "start_time": "2023-09-18T15:41:09.527415Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C2xd5vPwPVCb",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:41:18.337296Z",
     "start_time": "2023-09-18T15:41:11.611180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic_control,toy_text] in /usr/local/lib/python3.11/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/site-packages (from gymnasium[classic_control,toy_text]) (1.26.0rc1)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/site-packages (from gymnasium[classic_control,toy_text]) (2.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/site-packages (from gymnasium[classic_control,toy_text]) (4.8.0rc1)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/site-packages (from gymnasium[classic_control,toy_text]) (0.0.4)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/site-packages (from gymnasium[classic_control,toy_text]) (2.5.1)\r\n",
      "Requirement already satisfied: joblist in /usr/local/lib/python3.11/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: w3lib in /usr/local/lib/python3.11/site-packages (from joblist) (2.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "# Install gymnasium if you didn't\n",
    "!pip3 install \"gymnasium[toy_text,classic_control]\"\n",
    "!pip3 install joblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T15:41:22.653212Z",
     "start_time": "2023-09-18T15:41:20.330946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "springchallenge2023 imported\n",
      "PyLeagueEnv __init__\n"
     ]
    }
   ],
   "source": [
    "from springchallenge2023.wrappers.EncodeCellType import EncodeCellType\n",
    "from springchallenge2023.wrappers.ComputeEggCrystalRatio import ComputeEggCrystalRatio\n",
    "from springchallenge2023.wrappers.Normalize import Normalize\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# if you see \"<classname> has no attribute .env\", remove .env or update gym\n",
    "env = gym.make(\"springchallenge2023/PyLeague-v0\", max_episode_steps=100, render_mode='human')\n",
    "env = EncodeCellType(env)                # obs (15, 11) int,   action (15,)\n",
    "env = ComputeEggCrystalRatio(env)        # { map = obs (15, 13) float, ratio = float }, action (15,)\n",
    "env = Normalize(env)                     # { map = obs (15, 13) float, ratio = float }, action (15,)\n",
    "env = FlattenObservation(env)            # obs (121, ) float, action (15,)\n",
    "seed = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_2zbc7ahG9zK",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:41:24.797645Z",
     "start_time": "2023-09-18T15:41:24.780120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state vector dim = 167\n",
      "n_actions = Box(0.0, 1.0, (15,), float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.reset(seed=seed)\n",
    "n_actions = env.action_space\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# plt.imshow(env.render())\n",
    "print(\"state vector dim =\", state_dim)\n",
    "print(\"n_actions =\", n_actions)\n",
    "\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z72_alhdG9zK"
   },
   "source": [
    "# Neural Network Policy\n",
    "\n",
    "For this assignment we'll utilize the simplified neural network implementation from __[Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)__. Here's what you'll need:\n",
    "\n",
    "* `agent.partial_fit(states, actions)` - make a single training pass over the data. Maximize the probability of :actions: from :states:\n",
    "* `agent.predict_proba(states)` - predict probabilities of all actions, a matrix of shape __[len(states), n_actions]__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wLItY4unG9zL",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:42:43.845724Z",
     "start_time": "2023-09-18T15:42:43.716549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env0:  (122,)\n",
      "aenv0:  [array([1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ,\n",
      "       0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. ,\n",
      "       0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
      "       0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. , 1. , 0. ,\n",
      "       0. , 0. , 1. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. ,\n",
      "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 1. , 0. ,\n",
      "       0. , 0. , 0. , 1. , 0. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. ,\n",
      "       0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
      "       1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. , 0. , 0. ,\n",
      "       0. , 0. , 0. , 0.5, 1. ])]\n",
      "n_actions:  Box(0.0, 1.0, (15,), float64)\n",
      "act0:  [0.81901306 0.87807312 0.91603933 0.7096735  0.72917284 0.46048125\n",
      " 0.20609994 0.33649812 0.58794001 0.62654335 0.97636543 0.9534575\n",
      " 0.07250338 0.62470721 0.5623463 ]\n",
      "act0.shape:  (15,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([[0.81901306, 0.87807312, 0.91603933, 0.7096735 , 0.72917284,\n        0.46048125, 0.20609994, 0.33649812, 0.58794001, 0.62654335,\n        0.97636543, 0.9534575 , 0.07250338, 0.62470721, 0.5623463 ]]),)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mact0.shape: \u001B[39m\u001B[38;5;124m\"\u001B[39m, act0\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# initialize agent to the dimension of state space and number of actions\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43menv0\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mact0\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/sklearn/base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1149\u001B[0m     )\n\u001B[1;32m   1150\u001B[0m ):\n\u001B[0;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1209\u001B[0m, in \u001B[0;36mMLPClassifier.partial_fit\u001B[0;34m(self, X, y, classes)\u001B[0m\n\u001B[1;32m   1206\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1207\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_label_binarizer\u001B[38;5;241m.\u001B[39mfit(classes)\n\u001B[0;32m-> 1209\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincremental\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:442\u001B[0m, in \u001B[0;36mBaseMultilayerPerceptron._fit\u001B[0;34m(self, X, y, incremental)\u001B[0m\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    436\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhidden_layer_sizes must be > 0, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m hidden_layer_sizes\n\u001B[1;32m    437\u001B[0m     )\n\u001B[1;32m    438\u001B[0m first_pass \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoefs_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarm_start \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m incremental\n\u001B[1;32m    440\u001B[0m )\n\u001B[0;32m--> 442\u001B[0m X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mincremental\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfirst_pass\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    444\u001B[0m n_samples, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    446\u001B[0m \u001B[38;5;66;03m# Ensure y is 2D\u001B[39;00m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1127\u001B[0m, in \u001B[0;36mMLPClassifier._validate_input\u001B[0;34m(self, X, y, incremental, reset)\u001B[0m\n\u001B[1;32m   1125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_label_binarizer\u001B[38;5;241m.\u001B[39mclasses_\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1127\u001B[0m     classes \u001B[38;5;241m=\u001B[39m \u001B[43munique_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarm_start:\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(classes) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_):\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/sklearn/utils/multiclass.py:104\u001B[0m, in \u001B[0;36munique_labels\u001B[0;34m(*ys)\u001B[0m\n\u001B[1;32m    102\u001B[0m _unique_labels \u001B[38;5;241m=\u001B[39m _FN_UNIQUE_LABELS\u001B[38;5;241m.\u001B[39mget(label_type, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _unique_labels:\n\u001B[0;32m--> 104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown label type: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mrepr\u001B[39m(ys))\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_array_api_compliant:\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;66;03m# array_api does not allow for mixed dtypes\u001B[39;00m\n\u001B[1;32m    108\u001B[0m     unique_ys \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39mconcat([_unique_labels(y) \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m ys])\n",
      "\u001B[0;31mValueError\u001B[0m: Unknown label type: (array([[0.81901306, 0.87807312, 0.91603933, 0.7096735 , 0.72917284,\n        0.46048125, 0.20609994, 0.33649812, 0.58794001, 0.62654335,\n        0.97636543, 0.9534575 , 0.07250338, 0.62470721, 0.5623463 ]]),)"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "agent = MLPClassifier(hidden_layer_sizes=(200, 100),\n",
    "                     activation='relu',\n",
    "                     max_iter=10000,\n",
    "                     solver='sgd',\n",
    "                     alpha=0.0001,\n",
    "                     learning_rate='invscaling')\n",
    "\n",
    "\n",
    "env0=env.reset(seed=seed)[0]\n",
    "act0 = n_actions.sample()\n",
    "print(\"env0: \", env0.shape)\n",
    "print(\"aenv0: \", [env0])\n",
    "print(\"n_actions: \", n_actions)\n",
    "print(\"act0: \", act0)\n",
    "print(\"act0.shape: \", act0.shape)\n",
    "\n",
    "# initialize agent to the dimension of state space and number of actions\n",
    "agent.partial_fit([env0], [act0], classes=[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:01.368003Z",
     "start_time": "2023-09-18T15:21:01.308792Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pyleague_mplcregressor.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpyleague_mplcregressor.joblib\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# load model\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m agent \u001B[38;5;241m=\u001B[39m \u001B[43mjoblib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    648\u001B[0m         obj \u001B[38;5;241m=\u001B[39m _unpickle(fobj)\n\u001B[1;32m    649\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 650\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    651\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m _read_fileobject(f, filename, mmap_mode) \u001B[38;5;28;01mas\u001B[39;00m fobj:\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    653\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[1;32m    654\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[1;32m    655\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'pyleague_mplcregressor.joblib'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "filename = \"pyleague_mplcregressor.joblib\"\n",
    "\n",
    "# load model\n",
    "agent = joblib.load(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def normalize_0_to_1(data):\n",
    "    min_value = np.min(data)\n",
    "    max_value = np.max(data)\n",
    "    normalized_data = (data - min_value) / (max_value - min_value)\n",
    "    return normalized_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:04.595132Z",
     "start_time": "2023-09-18T15:21:04.576581Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eyFS3oUmG9zL",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:07.389879Z",
     "start_time": "2023-09-18T15:21:07.376409Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_session(env, agent, t_max=100):\n",
    "    \"\"\"\n",
    "    Play a single game using agent neural network.\n",
    "    Terminate when game finishes or after :t_max: steps\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0\n",
    "    debug = False\n",
    "    \n",
    "    s, _ = env.reset(seed=seed)\n",
    "    if debug:\n",
    "        env.render()\n",
    "\n",
    "    for t in range(t_max):\n",
    "\n",
    "        # print(\"action_space \", env.action_space.n)\n",
    "        # print(\"state \",s)\n",
    "        # print(\"n_actions\", n_actions)\n",
    "\n",
    "        # use agent to predict a vector of action probabilities for state :s:\n",
    "        act_predict = agent.predict(s.reshape(1, -1)).reshape(n_actions.shape,)\n",
    "\n",
    "        # Update state based on action\n",
    "        # act = np.exp(act_predict)\n",
    "        # act = normalize_0_to_1(act_predict)\n",
    "        act = act_predict\n",
    "        \n",
    "\n",
    "        # print(\"act \",act)\n",
    "        # print(\"act.shape \",act.shape)\n",
    "        # print(\"n_actions.shape \",n_actions.shape)\n",
    "        assert act.shape == n_actions.shape, \"make sure probabilities are a vector (hint: np.reshape)\"\n",
    "\n",
    "        # use the probabilities you predicted to pick an action\n",
    "        # sample proportionally to the probabilities, don't just take the most likely action\n",
    "        # a = np.random.choice(range(n_actions), p=probs)\n",
    "        # ^-- hint: try np.random.choice\n",
    "\n",
    "        new_s, r, terminated, truncated, _ = env.step(act)\n",
    "\n",
    "        # record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(act)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    if debug:\n",
    "        env.render()\n",
    "\n",
    "    return states, actions, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4xgrTCgJG9zL",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:08.449542Z",
     "start_time": "2023-09-18T15:21:08.335196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: [[1.         0.         0.         ... 0.         0.5        1.        ]\n",
      " [1.         0.         0.         ... 0.         0.5        1.        ]\n",
      " [1.         0.         0.         ... 0.         0.52173913 1.        ]\n",
      " ...\n",
      " [1.         0.         0.         ... 0.         0.75949367 1.        ]\n",
      " [1.         0.         0.         ... 0.         0.84507042 1.        ]\n",
      " [1.         0.         0.         ... 0.         0.95238095 1.        ]]\n",
      "actions: [array([-0.25930846, -0.01340216,  0.82369361,  0.30893909, -0.10794059,\n",
      "       -0.13608738,  0.077753  ,  0.19112516,  0.26238273,  0.29597095,\n",
      "        0.28596425,  0.41325483,  0.1653012 , -0.79575312,  0.07029193]), array([-0.50598663, -0.18069667,  0.7350976 ,  0.28373717, -0.03496854,\n",
      "        0.05053743,  0.08999139,  0.12794326,  0.2100201 ,  0.28522994,\n",
      "        0.43981603,  0.23274027, -0.03244182, -0.98560542,  0.01376597]), array([-0.39494304, -0.18623685,  0.68097203,  0.32124288, -0.04589894,\n",
      "       -0.16311131,  0.00666493,  0.07686803,  0.14440113,  0.25231177,\n",
      "        0.43154751,  0.28731468, -0.12790056, -0.64254922,  0.2359478 ]), array([-0.41249221, -0.19593157,  0.68245488,  0.31755685, -0.02735155,\n",
      "       -0.13778617,  0.05165795,  0.18772549,  0.03654478,  0.32399161,\n",
      "        0.42245502,  0.29608915, -0.09321754, -0.80201801,  0.13333314]), array([-0.25471166, -0.27515028,  0.72377841,  0.27190445, -0.22664717,\n",
      "       -0.10351878,  0.13020894,  0.21397569,  0.22918681,  0.0574185 ,\n",
      "        0.41615013,  0.27778298,  0.00841801, -0.71561326,  0.0761372 ]), array([-0.39150258, -0.18193958,  0.89187551,  0.44417002, -0.27043709,\n",
      "       -0.18247411,  0.1549879 ,  0.1627401 ,  0.2749085 ,  0.23568223,\n",
      "        0.34648374,  0.32011871,  0.12703793, -0.86286629,  0.04682345]), array([-0.353578  , -0.09732632,  0.67131968,  0.29213321, -0.01444652,\n",
      "       -0.03540111, -0.02130533,  0.1720098 ,  0.11821367,  0.23640302,\n",
      "        0.44917171,  0.29839404,  0.01457688, -0.83675598,  0.07640922]), array([-0.36194516, -0.0704945 ,  0.67436699,  0.27719939, -0.02410903,\n",
      "       -0.02061861, -0.07165213,  0.18038083,  0.11814221,  0.2339317 ,\n",
      "        0.43766834,  0.29924927, -0.00225579, -0.85104834,  0.05717504]), array([-0.35615781, -0.05379528,  0.69074602,  0.28533962, -0.03338624,\n",
      "       -0.02458556, -0.08290418,  0.180991  ,  0.12453634,  0.23383818,\n",
      "        0.44351277,  0.30027012,  0.00235477, -0.85310218,  0.06922327]), array([-0.36014409, -0.02853742,  0.70563345,  0.26400429, -0.03190899,\n",
      "       -0.01983657, -0.08723303,  0.18395781,  0.12534877,  0.23668833,\n",
      "        0.45024406,  0.30314858,  0.01011323, -0.85256607,  0.07858769]), array([-0.37035956, -0.06424714,  0.71073577,  0.27248598, -0.02601111,\n",
      "       -0.0263705 , -0.09186769,  0.18520203,  0.1296544 ,  0.25611608,\n",
      "        0.45880153,  0.29287496,  0.01933883, -0.84571587,  0.05223778])]\n",
      "reward: -155\n"
     ]
    }
   ],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=100000000)\n",
    "print(\"states:\", np.stack(dummy_states))\n",
    "print(\"actions:\", dummy_actions)\n",
    "print(\"reward:\", dummy_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p85lt16qG9zL"
   },
   "source": [
    "### CEM steps\n",
    "Deep CEM uses exactly the same strategy as the regular CEM, so you can copy your function code from previous notebook.\n",
    "\n",
    "The only difference is that now each observation is not a number but a `float32` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4On-p7p4G9zL",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:12.279614Z",
     "start_time": "2023-09-18T15:21:12.106824Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order\n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "    debug = False\n",
    "    if debug:\n",
    "        print(\"states_batch \",states_batch)\n",
    "        print(\"actions_batch \",actions_batch)\n",
    "        print(\"rewards_batch \",rewards_batch)\n",
    "        print(\"percentile \",percentile)\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    if debug:\n",
    "        print(\"reward_threshold \", reward_threshold)\n",
    "\n",
    "    reward_select = np.zeros_like(rewards_batch)\n",
    "    reward_select[rewards_batch >= reward_threshold] = 1\n",
    "    if debug:\n",
    "        print(\"reward_select \", reward_select)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "\n",
    "    for index, item in enumerate(reward_select):\n",
    "        if item:\n",
    "            elite_states.extend(states_batch[index])\n",
    "            elite_actions.extend(actions_batch[index])\n",
    "\n",
    "    if debug:\n",
    "        print(\"elite_states \", elite_states)\n",
    "        print(\"elite_actions \", elite_actions)\n",
    "\n",
    "    return elite_states, elite_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:13.828686Z",
     "start_time": "2023-09-18T15:21:13.747846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "states_batch = [\n",
    "    [[-0.1, -0.1,  0.1,  0.1],\n",
    "    [-0.2, -0.2,  0.2,  0.2],\n",
    "    [-0.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-0.4, -0.4,   0.4,  0.4  ],\n",
    "    [-0.5, -0.5,  0.5,  0.5 ]] ,  # game1\n",
    "    \n",
    "    [[-1.1, -0.1,  0.1,  0.1],\n",
    "    [-1.2, -0.2,  0.2,  0.2],\n",
    "    [-1.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-1.4, -0.4,   0.4,  0.4  ],\n",
    "    [-1.5, -0.5,  0.5,  0.5 ]] ,  # game2\n",
    "        \n",
    "    [[-2.1, -0.1,  0.1,  0.1],\n",
    "    [-2.2, -0.2,  0.2,  0.2],\n",
    "    [-2.3,  -0.3,  0.3,  0.3 ]],   # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 0, 0, 0, 1],     # game1\n",
    "    [0, 1, 0, 1, 1],  # game2\n",
    "    [0, 0, 0],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    5.0,  # game1\n",
    "    5.0,  # game2\n",
    "    3.0,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30\n",
    ")\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100\n",
    ")\n",
    "\n",
    "assert np.all(\n",
    "    test_result_0[0] == [\n",
    "    [-0.1, -0.1,  0.1,  0.1],\n",
    "    [-0.2, -0.2,  0.2,  0.2],\n",
    "    [-0.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-0.4, -0.4,   0.4,  0.4  ],\n",
    "    [-0.5, -0.5,  0.5,  0.5 ]\n",
    "    ,\n",
    "    [-1.1, -0.1,  0.1,  0.1],\n",
    "    [-1.2, -0.2,  0.2,  0.2],\n",
    "    [-1.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-1.4, -0.4,   0.4,  0.4  ],\n",
    "    [-1.5, -0.5,  0.5,  0.5 ]\n",
    "    ,\n",
    "    [-2.1, -0.1,  0.1,  0.1],\n",
    "    [-2.2, -0.2,  0.2,  0.2],\n",
    "    [-2.3,  -0.3,  0.3,  0.3 ]\n",
    "    ]\n",
    "    and test_result_0[1] == [\n",
    "        0, 0, 0, 0, 1,     # game1\n",
    "        0, 1, 0, 1, 1,  # game2\n",
    "        0, 0, 0   # game2\n",
    "    ]\n",
    "), \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "\n",
    "\n",
    "assert np.all(\n",
    "    test_result_30[0] == [\n",
    "    [-0.1, -0.1,  0.1,  0.1],\n",
    "    [-0.2, -0.2,  0.2,  0.2],\n",
    "    [-0.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-0.4, -0.4,   0.4,  0.4  ],\n",
    "    [-0.5, -0.5,  0.5,  0.5 ]\n",
    "    ,\n",
    "    [-1.1, -0.1,  0.1,  0.1],\n",
    "    [-1.2, -0.2,  0.2,  0.2],\n",
    "    [-1.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-1.4, -0.4,   0.4,  0.4  ],\n",
    "    [-1.5, -0.5,  0.5,  0.5 ]\n",
    "    ]\n",
    "    and test_result_30[1] == [\n",
    "        0, 0, 0, 0, 1,     # game1\n",
    "        0, 1, 0, 1, 1  # game2\n",
    "    ]\n",
    "), \"For percentile 30 you should only select states/actions from two first\"\n",
    "\n",
    "assert np.all(\n",
    "    test_result_100[0] == [\n",
    "    [-0.1, -0.1,  0.1,  0.1],\n",
    "    [-0.2, -0.2,  0.2,  0.2],\n",
    "    [-0.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-0.4, -0.4,   0.4,  0.4  ],\n",
    "    [-0.5, -0.5,  0.5,  0.5 ]\n",
    "    ,\n",
    "    [-1.1, -0.1,  0.1,  0.1],\n",
    "    [-1.2, -0.2,  0.2,  0.2],\n",
    "    [-1.3,  -0.3,  0.3,  0.3 ],\n",
    "    [-1.4, -0.4,   0.4,  0.4  ],\n",
    "    [-1.5, -0.5,  0.5,  0.5 ]\n",
    "    ]\n",
    "    and test_result_100[1] == [\n",
    "        0, 0, 0, 0, 1,     # game1\n",
    "        0, 1, 0, 1, 1  # game2\n",
    "    ]\n",
    "), \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "\n",
    "print(\"Ok!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc40V4DaG9zM"
   },
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PPwVKwF7G9zM",
    "ExecuteTime": {
     "end_time": "2023-09-18T15:21:16.388714Z",
     "start_time": "2023-09-18T15:21:16.246741Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress.\n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    print(\"date %s\" % (datetime.now()))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label=\"Mean rewards\")\n",
    "    plt.plot(list(zip(*log))[1], label=\"Reward thresholds\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines(\n",
    "        [np.percentile(rewards_batch, percentile)],\n",
    "        [0],\n",
    "        [100],\n",
    "        label=\"percentile\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeNWKjtsG9zM"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = -92.000, threshold=-92.000\n",
      "date 2023-09-18 17:38:21.622749\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x400 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m log \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# generate new sessions\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m     sessions \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerate_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_max\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mn_sessions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m         pprint(sessions)\n",
      "Cell \u001B[0;32mIn[29], line 8\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m log \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# generate new sessions\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m     sessions \u001B[38;5;241m=\u001B[39m [ \u001B[43mgenerate_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_max\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_sessions) ]\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m         pprint(sessions)\n",
      "Cell \u001B[0;32mIn[17], line 39\u001B[0m, in \u001B[0;36mgenerate_session\u001B[0;34m(env, agent, t_max)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m act\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m n_actions\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake sure probabilities are a vector (hint: np.reshape)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# use the probabilities you predicted to pick an action\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# sample proportionally to the probabilities, don't just take the most likely action\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# a = np.random.choice(range(n_actions), p=probs)\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# ^-- hint: try np.random.choice\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m new_s, r, terminated, truncated, _ \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mact\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# record sessions like you did before\u001B[39;00m\n\u001B[1;32m     42\u001B[0m states\u001B[38;5;241m.\u001B[39mappend(s)\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:522\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[1;32m    520\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    521\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:522\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[1;32m    520\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    521\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "    \u001B[0;31m[... skipping similar frames: ObservationWrapper.step at line 522 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/core.py:522\u001B[0m, in \u001B[0;36mObservationWrapper.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\n\u001B[1;32m    519\u001B[0m     \u001B[38;5;28mself\u001B[39m, action: ActType\n\u001B[1;32m    520\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[WrapperObsType, SupportsFloat, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]]:\n\u001B[1;32m    521\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001B[0m, in \u001B[0;36mTimeLimit.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action):\n\u001B[1;32m     47\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m     observation, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_elapsed_steps \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_max_episode_steps:\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling env.reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001B[0m, in \u001B[0;36mPassiveEnvChecker.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_step_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, action)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/gym-springchallenge2023/springchallenge2023/envs/py_league.py:73\u001B[0m, in \u001B[0;36mstep\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     71\u001B[0m nactions \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mint\u001B[39m(value\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m10\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m value \u001B[38;5;129;01min\u001B[39;00m action]\n\u001B[1;32m     72\u001B[0m cmd0 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEACON \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(nactions) \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 73\u001B[0m cmd1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEACON \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m 1\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m15\u001B[39m)])\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgame\u001B[38;5;241m.\u001B[39mhandle_player_commands(player0, cmd0)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgame\u001B[38;5;241m.\u001B[39mhandle_player_commands(player1, cmd1)\n",
      "File \u001B[0;32m~/Projects/gym-springchallenge2023/springchallenge2023/pyleague/game/Game.py:68\u001B[0m, in \u001B[0;36mGame.handle_player_commands\u001B[0;34m(self, player, command)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhandle_player_commands\u001B[39m(\u001B[38;5;28mself\u001B[39m, player: Player, command: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m---> 68\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_commands\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/gym-springchallenge2023/springchallenge2023/pyleague/game/CommandManager.py:21\u001B[0m, in \u001B[0;36mCommandManager.parse_commands\u001B[0;34m(self, player, line)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action_type \u001B[38;5;129;01min\u001B[39;00m ActionType:\n\u001B[1;32m     20\u001B[0m     pattern \u001B[38;5;241m=\u001B[39m action_type\u001B[38;5;241m.\u001B[39mget_pattern()\n\u001B[0;32m---> 21\u001B[0m     match \u001B[38;5;241m=\u001B[39m \u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m match:\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m action_type \u001B[38;5;241m==\u001B[39m ActionType\u001B[38;5;241m.\u001B[39mBEACON:\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/re/__init__.py:166\u001B[0m, in \u001B[0;36mmatch\u001B[0;34m(pattern, string, flags)\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmatch\u001B[39m(pattern, string, flags\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    164\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001B[39;00m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_compile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpattern\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstring\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "n_sessions = 100\n",
    "percentile = 60\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # generate new sessions\n",
    "    sessions = [ generate_session(env, agent, t_max=10000) for _ in range(n_sessions) ]\n",
    "\n",
    "    if False:\n",
    "        pprint(sessions)\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = map(list, zip(*sessions))\n",
    "    if False:\n",
    "        print(states_batch.shape)\n",
    "        print(actions_batch.shape)\n",
    "        print(rewards_batch.shape)\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    if False:\n",
    "        print(elite_states)\n",
    "        print(elite_actions)\n",
    "    \n",
    "    #for a in elite_actions:\n",
    "    #     a[2] = 0\n",
    "    #     a[8] = 0\n",
    "    #     a[0] = 0.4\n",
    "    #     a[1] = 0.4\n",
    "    #     a[5] = 0.4\n",
    "    #     a[4] = 0.4\n",
    "\n",
    "    # <YOUR CODE: partial_fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.partial_fit(elite_states, elite_actions)\n",
    "    # save model\n",
    "    joblib.dump(agent, filename)\n",
    "\n",
    "    show_progress(\n",
    "         rewards_batch, log, percentile, reward_range=[np.min(rewards_batch), np.max(rewards_batch)]\n",
    "    )\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJwsWl4kG9zM",
    "ExecuteTime": {
     "start_time": "2023-09-18T15:19:46.993957Z"
    }
   },
   "outputs": [],
   "source": [
    "# Record sessions\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "with RecordVideo(\n",
    "    env=gym.make(\"CartPole-v0\", render_mode=\"rgb_array\", max_episode_steps=10000),\n",
    "    video_folder=\"./videos\",\n",
    "    episode_trigger=lambda episode_number: True,\n",
    "    video_length=0\n",
    ") as env_monitor:\n",
    "    sessions = [generate_session(env_monitor, agent,100000) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLPXdME7G9zN",
    "ExecuteTime": {
     "start_time": "2023-09-18T15:19:46.997236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_paths = sorted([s for s in Path(\"videos\").iterdir() if s.suffix == \".mp4\"])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open(\"rb\") as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\n",
    "    \"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\n",
    "        data_url\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d_3oOQ1G9zN"
   },
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular crossentropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -100 to a near-optimal score and then descends back into -50/-100. This is in part because the environment has some innate randomness. Namely, the starting points of passenger/driver change from episode to episode.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (2 pts) Find out how the algorithm performance changes if you use a different `percentile` and/or `n_sessions`. Provide here some figures so we can see how the hyperparameters influence the performance.\n",
    "- __1.2__ (1 pts) Tune the algorithm to end up with positive average score.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L88LySiVG9zN"
   },
   "source": [
    "```<Describe what you did here>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LpAJc4rG9zN"
   },
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment, you should have got enough score on [CartPole-v0](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to consider it solved (see the link). It's time to try something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ (3 pts) Pick one of environments: `MountainCar-v0` or `LunarLander-v2`.\n",
    "  * For MountainCar, get average reward of __at least -150__\n",
    "  * For LunarLander, get average reward of __at least +50__\n",
    "\n",
    "See the tips section below, it's kinda important.\n",
    "__Note:__ If your agent is below the target score, you'll still get some of the points depending on the result, so don't be afraid to submit it.\n",
    "  \n",
    "  \n",
    "* __2.2__ (up to 6 pts) Devise a way to speed up training against the default version\n",
    "  * Obvious improvement: use [`joblib`](https://joblib.readthedocs.io/en/latest/). However, note that you will probably need to spawn a new environment in each of the workers instead of passing it via pickling. (2 pts)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training. (2 pts)\n",
    "  * Obtain __-100__ at `MountainCar-v0` or __+200__ at `LunarLander-v2` (2 pts). Feel free to experiment with hyperparameters, architectures, schedules etc.\n",
    "  \n",
    "__Please list what you did in Anytask submission form__. This reduces probability that somebody misses something.\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gymnasium pages: [MountainCar](https://gymnasium.farama.org/environments/classic_control/mountain_car/), [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 10% are better, than if you use percentile 20% as threshold, R >= threshold __fails to cut off bad sessions__ while R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it doesn't train, it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "\n",
    "You may find the following snippet useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcjz-nm_G9zN",
    "ExecuteTime": {
     "start_time": "2023-09-18T15:19:47.000161Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_mountain_car(env, agent):\n",
    "    # Compute policy for all possible x and v (with discretization)\n",
    "    xs = np.linspace(env.min_position, env.max_position, 100)\n",
    "    vs = np.linspace(-env.max_speed, env.max_speed, 100)\n",
    "\n",
    "    grid = np.dstack(np.meshgrid(xs, vs[::-1])).transpose(1, 0, 2)\n",
    "    grid_flat = grid.reshape(len(xs) * len(vs), 2)\n",
    "    probs = (\n",
    "        agent.predict_proba(grid_flat).reshape(len(xs), len(vs), 3).transpose(1, 0, 2)\n",
    "    )\n",
    "\n",
    "    # # The above code is equivalent to the following:\n",
    "    # probs = np.empty((len(vs), len(xs), 3))\n",
    "    # for i, v in enumerate(vs[::-1]):\n",
    "    #     for j, x in enumerate(xs):\n",
    "    #         probs[i, j, :] = agent.predict_proba([[x, v]])[0]\n",
    "\n",
    "    # Draw policy\n",
    "    f, ax = plt.subplots(figsize=(7, 7))\n",
    "    ax.imshow(\n",
    "        probs,\n",
    "        extent=(env.min_position, env.max_position, -env.max_speed, env.max_speed),\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "    ax.set_title(\"Learned policy: red=left, green=nothing, blue=right\")\n",
    "    ax.set_xlabel(\"position (x)\")\n",
    "    ax.set_ylabel(\"velocity (v)\")\n",
    "\n",
    "    # Sample a trajectory and draw it\n",
    "    states, actions, _ = generate_session(env, agent)\n",
    "    states = np.array(states)\n",
    "    ax.plot(states[:, 0], states[:, 1], color=\"white\")\n",
    "\n",
    "    # Draw every 3rd action from the trajectory\n",
    "    for (x, v), a in zip(states[::3], actions[::3]):\n",
    "        if a == 0:\n",
    "            plt.arrow(x, v, -0.1, 0, color=\"white\", head_length=0.02)\n",
    "        elif a == 2:\n",
    "            plt.arrow(x, v, 0.1, 0, color=\"white\", head_length=0.02)\n",
    "\n",
    "\n",
    "with gym.make(\"MountainCar-v0\", render_mode=\"rgb_arrary\").env as env:\n",
    "    visualize_mountain_car(env, agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzk41lDPG9zO"
   },
   "source": [
    "### Bonus tasks\n",
    "\n",
    "* __2.3 bonus__ (2 pts) Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation. If you attempted this task, please mention it in Anytask submission._)\n",
    "\n",
    "* __2.4 bonus__ (4 pts) Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * Since your agent only predicts the \"expected\" action, you will have to add noise to ensure exploration.\n",
    "  * Choose one of [MountainCarContinuous-v0](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/) (90+ pts to solve), [LunarLanderContinuous-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/) (`env = gym.make(\"LunarLander-v2\", continuous=True)`)(200+ pts to solve)\n",
    "  * 4 points for solving. Slightly less for getting some results below solution threshold. Note that discrete and continuous environments may have slightly different rules, aside from action spaces."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
